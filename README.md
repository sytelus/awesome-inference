# awesome-inference
 Deep Learning space is now exploding with all the new inference frameworks. This is the repo to keep track of active list. The initial list was seeded from [Model_Inference_Deployment](https://github.com/sytelus/Model_Inference_Deployment) and you still might want to check it out if this list is outdated.

 Please add pull request for updates.


* [ONNX Runtime](https://github.com/onnx) - By Microsoft as an optimized runtime for exported models in standard ONNX format.
* [DeepSpeed-MII](https://github.com/microsoft/deepspeed-mii) - Inference library that makes 24000 models, including very large one, fast through deepfusion, tensor-slicing, ZeroQuant, distributing model on GPU and CPU etc.
* [OpenAI Triton](https://github.com/ELS-RD/kernl/) - Kernels written in OpenAI Triton language to make models run fast on GPU.
* [AI Template](https://github.com/facebookincubator/AITemplate) - By Meta AI, framework to run models on NVidia as well as AMD GPUs.
* [OpenVINO](https://github.com/openvinotoolkit/openvino) - By Intel to run models fast on Intel CPUs.
* [TensorRT](https://github.com/NVIDIA/TensorRT) - By NVidia to run models fast on NVidia GPUs.
* [TVM](https://github.com/apache/tvm) - Generate optimized tensor operators for given hardware.
* [MediaPipe](https://github.com/google/mediapipe) - Gallery of models for popular tasks optimized for iOS, Android and more.
* [TensorFlow Lite](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite) - Runtime for mobile and embedded devices for TensorFlow models.
* [TensorFlow Serving](https://github.com/tensorflow/serving) - Serving infrastructure for TensorFlow model.
* [LibTorch](https://pytorch.org/cppdocs/installing.html) -  C++ distributions of PyTorch.
* [NCNN](https://github.com/Tencent/ncnn) - By Tencent, inference framework for mobile devices used by their own apps.
* [TNN](https://github.com/Tencent/TNN) - By Tencent, cross platform acceleration for many tasks in their app.
* [MNN](https://github.com/alibaba/MNN) - By Alibaba, framework for compression, training and serving used by 30 apps in Alibaba.
* [MACE](https://github.com/XiaoMi/mace) - By XiaoMi, Mobile AI Compute Engine, inference framework for mobile.
* [Paddle Lite](https://github.com/PaddlePaddle/Paddle-Lite) - PaddlePaddle, inference on mobile and IoT
* [MegEngine Lite](https://github.com/MegEngine/MegEngine/tree/master/lite) - for MegEngine.
* [OpenPPL](https://github.com/openppl-public/ppl.nn) - Primitive Library for Neural Network, runs ONNX models on GPU and CPU.
* [Bolt](https://github.com/huawei-noah/bolt) - By Huawei, can run OONX, TFLite models, claims to be 15% faster than others supporting Qualcomm GPU, Mali GPU and CPUs.
